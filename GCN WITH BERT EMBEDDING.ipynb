{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNV0yMC5OmR81rChfMZGsNq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Y5XReab9EBe2"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertModel  # 1. Import BERT-related modules\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import re\n","from nltk.corpus import stopwords\n","import nltk\n","\n","nltk.download('stopwords')\n","\n","# Load NLTK stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Define preprocessing function\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    text = ' '.join(word for word in text.split() if word not in stop_words)\n","    return text\n","\n","# Load and preprocess data\n","train_data = pd.read_csv('/content/restrain.csv')\n","test_data = pd.read_csv('/content/restest.csv')\n","\n","train_data.dropna(inplace=True)\n","test_data.dropna(inplace=True)\n","\n","train_data['Sentence'] = train_data['Sentence'].apply(preprocess_text)\n","test_data['Sentence'] = test_data['Sentence'].apply(preprocess_text)\n","\n","# Initialize BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Initialize BERT tokenizer\n","\n","# Tokenize text data with BERT tokenizer\n","train_encodings = tokenizer(train_data['Sentence'].tolist(), truncation=True, padding=True)\n","test_encodings = tokenizer(test_data['Sentence'].tolist(), truncation=True, padding=True)\n","\n","# Load and preprocess labels\n","encoder = LabelEncoder()\n","train_labels = encoder.fit_transform(train_data['Polarity'])\n","test_labels = encoder.transform(test_data['Polarity'])\n","\n","# Define custom dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        return {'input_ids': torch.tensor(self.encodings['input_ids'][idx], dtype=torch.long),\n","                'attention_mask': torch.tensor(self.encodings['attention_mask'][idx], dtype=torch.long),\n","                'labels': torch.tensor(self.labels[idx], dtype=torch.long)}\n","\n","# Create datasets and data loaders\n","train_dataset = CustomDataset(train_encodings, train_labels)\n","test_dataset = CustomDataset(test_encodings, test_labels)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Define GCN model with BERT embeddings\n","class GCN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(GCN, self).__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')  # Load BERT model\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.pooler_output\n","        x = torch.relu(self.fc1(pooled_output))\n","        x = self.fc2(x)\n","        return torch.log_softmax(x, dim=1)\n","\n","# Define training function\n","def train(model, train_loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0.0\n","    for batch in train_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(train_loader)\n","\n","# Define evaluation function\n","def evaluate(model, test_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0.0\n","    correct = 0\n","    predictions = []\n","    ground_truths = []\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids, attention_mask)\n","            total_loss += criterion(outputs, labels).item()\n","            _, predicted = torch.max(outputs, 1)\n","            correct += (predicted == labels).sum().item()\n","            predictions.extend(predicted.cpu().numpy())\n","            ground_truths.extend(labels.cpu().numpy())\n","    accuracy = correct / len(test_loader.dataset)\n","    avg_loss = total_loss / len(test_loader)\n","    return avg_loss, accuracy, predictions, ground_truths\n","\n","# Define device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Initialize model, optimizer, and loss function\n","input_dim = 768  # BERT hidden size\n","hidden_dim = 128\n","output_dim = len(encoder.classes_)\n","model = GCN(input_dim, hidden_dim, output_dim).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=2e-5)  # BERT recommended learning rate\n","criterion = nn.CrossEntropyLoss()\n","\n","# Train and evaluate the model\n","num_epochs = 10\n","best_accuracy = 0.0\n","for epoch in range(1, num_epochs + 1):\n","    train_loss = train(model, train_loader, optimizer, criterion, device)\n","    test_loss, test_accuracy, predictions, ground_truths = evaluate(model, test_loader, criterion, device)\n","    print(f'Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n","\n","    # Calculate precision, recall, and F1 score\n","    precision = precision_score(ground_truths, predictions, average='weighted')\n","    recall = recall_score(ground_truths, predictions, average='weighted')\n","    f1 = f1_score(ground_truths, predictions, average='weighted')\n","    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n","\n","    # Save the model with the best accuracy\n","    if test_accuracy > best_accuracy:\n","        best_accuracy = test_accuracy\n","        torch.save(model.state_dict(), 'best_gcn_model.pth')\n","\n","# Save trained model\n","torch.save(model.state_dict(), 'gcn_model.pth')\n"]},{"cell_type":"markdown","source":["**2 Code **"],"metadata":{"id":"HW1vRdFrK-PR"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertModel\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import re\n","from nltk.corpus import stopwords\n","import nltk\n","\n","nltk.download('stopwords')\n","\n","# Load NLTK stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Define preprocessing function\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    text = ' '.join(word for word in text.split() if word not in stop_words)\n","    return text\n","\n","# Load and preprocess data\n","train_data = pd.read_csv('/content/tweet train (1).csv')\n","test_data = pd.read_csv('/content/tweet test (1).csv')\n","\n","train_data.dropna(inplace=True)\n","test_data.dropna(inplace=True)\n","\n","train_data['Sentence'] = train_data['Sentence'].apply(preprocess_text)\n","test_data['Sentence'] = test_data['Sentence'].apply(preprocess_text)\n","\n","# Initialize BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize text data with BERT tokenizer\n","train_encodings = tokenizer(train_data['Sentence'].tolist(), truncation=True, padding=True)\n","test_encodings = tokenizer(test_data['Sentence'].tolist(), truncation=True, padding=True)\n","\n","# Load and preprocess labels\n","encoder = LabelEncoder()\n","train_labels = encoder.fit_transform(train_data['Polarity'])\n","test_labels = encoder.transform(test_data['Polarity'])\n","\n","# Define custom dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        return {'input_ids': torch.tensor(self.encodings['input_ids'][idx], dtype=torch.long),\n","                'attention_mask': torch.tensor(self.encodings['attention_mask'][idx], dtype=torch.long),\n","                'labels': torch.tensor(self.labels[idx], dtype=torch.long)}\n","\n","# Create datasets and data loaders\n","train_dataset = CustomDataset(train_encodings, train_labels)\n","test_dataset = CustomDataset(test_encodings, test_labels)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Define GCN model with BERT embeddings\n","class GCN(nn.Module):\n","    def __init__(self, output_dim):\n","        super(GCN, self).__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        self.dropout = nn.Dropout(0.5)  # Increased dropout rate for regularization\n","        self.fc = nn.Linear(self.bert.config.hidden_size, output_dim)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.pooler_output\n","        x = self.dropout(pooled_output)\n","        x = self.fc(x)\n","        return torch.log_softmax(x, dim=1)\n","\n","# Define training function with gradient clipping\n","def train(model, train_loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0.0\n","    for batch in train_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping to prevent exploding gradients\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(train_loader)\n","\n","# Define evaluation function\n","def evaluate(model, test_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0.0\n","    correct = 0\n","    predictions = []\n","    ground_truths = []\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids, attention_mask)\n","            total_loss += criterion(outputs, labels).item()\n","            _, predicted = torch.max(outputs, 1)\n","            correct += (predicted == labels).sum().item()\n","            predictions.extend(predicted.cpu().numpy())\n","            ground_truths.extend(labels.cpu().numpy())\n","    accuracy = correct / len(test_loader.dataset)\n","    avg_loss = total_loss / len(test_loader)\n","    return avg_loss, accuracy, predictions, ground_truths\n","\n","# Define device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Initialize model, optimizer, and loss function\n","output_dim = len(encoder.classes_)\n","model = GCN(output_dim).to(device)\n","\n","# Adjust the learning rate and weight decay\n","optimizer = optim.AdamW([\n","    {'params': model.bert.parameters(), 'lr': 1e-5},  # Fine-tune BERT layers with a lower learning rate\n","    {'params': model.fc.parameters(), 'lr': 5e-5}     # Higher learning rate for the additional layers\n","], weight_decay=0.01)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# Implement learning rate scheduler with warm-up steps\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Adjust step_size and gamma as needed\n","\n","# Train and evaluate the model\n","num_epochs = 10\n","best_accuracy = 0.0\n","for epoch in range(1, num_epochs + 1):\n","    train_loss = train(model, train_loader, optimizer, criterion, device)\n","    test_loss, test_accuracy, predictions, ground_truths = evaluate(model, test_loader, criterion, device)\n","    print(f'Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n","\n","    # Calculate precision, recall, and F1 score\n","    precision = precision_score(ground_truths, predictions, average='weighted')\n","    recall = recall_score(ground_truths, predictions, average='weighted')\n","    f1 = f1_score(ground_truths, predictions, average='weighted')\n","    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n","\n","    # Adjust learning rate scheduler\n","    scheduler.step()\n","\n","    # Save the model with the best accuracy\n","    if test_accuracy > best_accuracy:\n","        best_accuracy = test_accuracy\n","        torch.save(model.state_dict(), 'best_gcn_model.pth')\n","\n","# Save trained model\n","torch.save(model.state_dict(), 'gcn_model.pth')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UeXmvz33JtPW","executionInfo":{"status":"ok","timestamp":1715490250461,"user_tz":-330,"elapsed":522057,"user":{"displayName":"Aman Sankhwar","userId":"07284817566629259602"}},"outputId":"571861e7-2658-4d60-d0a1-98d04869d10c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Train Loss: 0.9735, Test Loss: 0.8130, Test Accuracy: 0.6322\n","Precision: 0.6735, Recall: 0.6322, F1 Score: 0.6040\n","Epoch 2/10, Train Loss: 0.7969, Test Loss: 0.7782, Test Accuracy: 0.6588\n","Precision: 0.6574, Recall: 0.6588, F1 Score: 0.6574\n","Epoch 3/10, Train Loss: 0.6814, Test Loss: 0.7669, Test Accuracy: 0.6883\n","Precision: 0.6904, Recall: 0.6883, F1 Score: 0.6863\n","Epoch 4/10, Train Loss: 0.5615, Test Loss: 0.8448, Test Accuracy: 0.6588\n","Precision: 0.6766, Recall: 0.6588, F1 Score: 0.6589\n","Epoch 5/10, Train Loss: 0.4563, Test Loss: 0.8340, Test Accuracy: 0.6883\n","Precision: 0.6916, Recall: 0.6883, F1 Score: 0.6891\n","Epoch 6/10, Train Loss: 0.3337, Test Loss: 0.8403, Test Accuracy: 0.6972\n","Precision: 0.6973, Recall: 0.6972, F1 Score: 0.6973\n","Epoch 7/10, Train Loss: 0.3164, Test Loss: 0.8682, Test Accuracy: 0.6869\n","Precision: 0.6866, Recall: 0.6869, F1 Score: 0.6866\n","Epoch 8/10, Train Loss: 0.3001, Test Loss: 0.8996, Test Accuracy: 0.6869\n","Precision: 0.6873, Recall: 0.6869, F1 Score: 0.6870\n","Epoch 9/10, Train Loss: 0.2926, Test Loss: 0.9154, Test Accuracy: 0.6957\n","Precision: 0.6977, Recall: 0.6957, F1 Score: 0.6963\n","Epoch 10/10, Train Loss: 0.2742, Test Loss: 0.9307, Test Accuracy: 0.6883\n","Precision: 0.6869, Recall: 0.6883, F1 Score: 0.6873\n"]}]},{"cell_type":"markdown","source":["# Distilled **bert**"],"metadata":{"id":"GcaK8ev9M6h0"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import DistilBertTokenizer, DistilBertModel\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score\n","import re\n","from nltk.corpus import stopwords\n","import nltk\n","\n","nltk.download('stopwords')\n","\n","# Load NLTK stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Define preprocessing function\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    text = ' '.join(word for word in text.split() if word not in stop_words)\n","    return text\n","\n","# Load data\n","train_data = pd.read_csv('/content/restrain.csv')  # Replace with path to your training data\n","test_data = pd.read_csv('/content/restest.csv')    # Replace with path to your testing data\n","train_data.dropna(inplace=True)\n","test_data.dropna(inplace=True)\n","\n","train_data['Sentence'] = train_data['Sentence'].apply(preprocess_text)\n","test_data['Sentence'] = test_data['Sentence'].apply(preprocess_text)\n","\n","# Initialize DistilBERT tokenizer\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","# Tokenize text data with DistilBERT tokenizer\n","train_encodings = tokenizer(train_data['Sentence'].tolist(), truncation=True, padding=True)\n","test_encodings = tokenizer(test_data['Sentence'].tolist(), truncation=True, padding=True)\n","\n","# Label Encoding\n","encoder = LabelEncoder()\n","train_labels = encoder.fit_transform(train_data['Polarity'])\n","test_labels = encoder.transform(test_data['Polarity'])\n","\n","# Custom Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return item\n","\n","# Create datasets and data loaders\n","train_dataset = CustomDataset(train_encodings, train_labels)\n","test_dataset = CustomDataset(test_encodings, test_labels)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Define GCN model with DistilBERT embeddings\n","class GCN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(GCN, self).__init__()\n","        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs[0][:, 0, :]  # Use the [CLS] token embedding\n","        x = torch.relu(self.fc1(pooled_output))\n","        x = self.fc2(x)\n","        return torch.log_softmax(x, dim=1)\n","\n","# Define training function\n","def train(model, train_loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0.0\n","    for batch in train_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(train_loader)\n","\n","# Define evaluation function\n","def evaluate(model, test_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0.0\n","    predictions = []\n","    ground_truths = []\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids, attention_mask)\n","            total_loss += criterion(outputs, labels).item()\n","            _, predicted = torch.max(outputs, 1)\n","            predictions.extend(predicted.cpu().numpy())\n","            ground_truths.extend(labels.cpu().numpy())\n","    accuracy = accuracy_score(ground_truths, predictions)\n","    avg_loss = total_loss / len(test_loader)\n","    return avg_loss, accuracy, predictions, ground_truths\n","\n","# Define device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Initialize model, optimizer, and loss function\n","input_dim = 768  # DistilBERT hidden size\n","hidden_dim = 128\n","output_dim = len(np.unique(train_labels))\n","model = GCN(input_dim, hidden_dim, output_dim).to(device)\n","optimizer = optim.AdamW(model.parameters(), lr=5e-5)  # Recommended learning rate for DistilBERT\n","criterion = nn.CrossEntropyLoss()\n","\n","# Train and evaluate the model\n","num_epochs = 10\n","best_accuracy = 0.0\n","for epoch in range(1, num_epochs + 1):\n","    train_loss = train(model, train_loader, optimizer, criterion, device)\n","    test_loss, test_accuracy, _, _ = evaluate(model, test_loader, criterion, device)\n","    print(f'Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n","\n","    # Save the model with the best accuracy\n","    if test_accuracy > best_accuracy:\n","        best_accuracy = test_accuracy\n","        torch.save(model.state_dict(), 'best_gcn_model.pth')\n","\n","# Save trained model\n","torch.save(model.state_dict(), 'gcn_model.pth')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gkJbzEuPQGZw","executionInfo":{"status":"ok","timestamp":1715659832516,"user_tz":-330,"elapsed":171268,"user":{"displayName":"Aman Sankhwar","userId":"07284817566629259602"}},"outputId":"95b109f3-0f14-4281-f10d-fc8bf8a2de13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Train Loss: 0.7667, Test Loss: 0.5481, Test Accuracy: 0.7909\n","Epoch 2/10, Train Loss: 0.5480, Test Loss: 0.6345, Test Accuracy: 0.7685\n","Epoch 3/10, Train Loss: 0.4457, Test Loss: 0.6465, Test Accuracy: 0.7802\n","Epoch 4/10, Train Loss: 0.3796, Test Loss: 0.6271, Test Accuracy: 0.7659\n","Epoch 5/10, Train Loss: 0.3210, Test Loss: 0.6151, Test Accuracy: 0.7668\n","Epoch 6/10, Train Loss: 0.2676, Test Loss: 0.6443, Test Accuracy: 0.7784\n","Epoch 7/10, Train Loss: 0.2457, Test Loss: 0.6717, Test Accuracy: 0.7721\n","Epoch 8/10, Train Loss: 0.2347, Test Loss: 0.6966, Test Accuracy: 0.7784\n","Epoch 9/10, Train Loss: 0.2259, Test Loss: 0.7703, Test Accuracy: 0.7623\n","Epoch 10/10, Train Loss: 0.2098, Test Loss: 0.8504, Test Accuracy: 0.7507\n"]}]}]}